# related work

Most early VQA tasks used CNN to extract feature vectors and LSTM to generate description text as the basic framework, but there were different processing methods for feature fusion modules:

【Show, Attend and Tell: Neural Image Caption Generation with Visual Attention】address the challenge of generating contextually relevant and accurate image captions by introducing a visual attention mechanism. Their model utilizes both soft and hard attention to dynamically focus on pertinent parts of images, enhancing the relevance and detail of the generated captions. The model demonstrated superior performance on standard image captioning datasets such as Flickr8k and Microsoft COCO, setting new benchmarks for caption quality and adaptability.

【Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering】addressed the challenge of spatial reasoning in Visual Question Answering (VQA) tasks by proposing a new model that integrates deep learning and attention mechanisms, named the Spatial Memory Network (SMem-VQA). This model utilizes a multi-hop attention mechanism, iteratively focusing on key areas of the image through a memory network to address specific query requirements. Experiments on standard VQA datasets such as DAQUAR and VQA demonstrate that the SMem-VQA model outperforms existing methods in complex spatial reasoning tasks, particularly in terms of accuracy and relevance to the questions posed.

【Structured Attentions for Visual Question Answering】tackle the limited effective receptive field in CNNs for Visual Question Answering (VQA), which struggles with complex spatial relationships between image regions. They introduce a novel visual attention model using a grid-structured Conditional Random Field (CRF) to capture inter-region dependencies, implemented through Mean Field and Loopy Belief Propagation algorithms as recurrent neural network layers. The model significantly improves upon baseline models, enhancing accuracy on the CLEVR dataset by 9.5% and on the VQA dataset by 1.25%.
In these papers, the authors use their own methods to use attention mechanisms in the image representation process to guide the model to select more accurate visual evidence to answer questions.

For some non-differentiable metrics, the training process can be adjusted in a feedback manner through the mechanism of self-criticism sequence to achieve better performance:

【Self-critical Sequence Training for Image Captioning】tackle the optimization of non-differentiable metrics in image caption generation, which traditional methods struggle with due to exposure bias and reward sparsity. They employ the REINFORCE algorithm enhanced with a self-critical sequence training strategy, using the model's own output at test time as a baseline for reward normalization. This approach led to significant improvements in captioning performance on the MSCOCO dataset, establishing a new state-of-the-art by effectively training on direct evaluation metrics.

In addition, there are also solutions for text modeling using CNN:

【Learning Convolutional Text Representations for Visual Question Answering】address the challenge of learning text representations for Visual Question Answering (VQA) tasks, critiquing traditional reliance on Recurrent Neural Networks (RNNs) and proposing the use of Convolutional Neural Networks (CNNs) for more effective text processing. They introduce a novel model named “CNN Inception + Gate,” which incorporates Inception modules and gating mechanisms to extract text features. This model utilizes convolutional kernels of varying sizes to capture essential textual information and employs gates to optimize the flow of information. Experimental results demonstrate that this CNN-based model not only improves the quality of question representations but also significantly enhances overall VQA accuracy, with fewer parameters and faster computation compared to traditional RNN-based models.

At the same time, the bidirectional attention mechanism in the field of machine understanding provides new ideas for text encoding:

【Bi-Directional Attention Flow for Machine Comprehension】aimed to enhance the comprehension capabilities of machine learning models in question-answering systems by addressing limitations in understanding the context of questions and answers. They introduced the Bi-Directional Attention Flow (BiDAF) model, which employs a novel bi-directional attention mechanism that enriches the mutual interaction between the context and the query for more accurate comprehension. BiDAF demonstrated superior performance, setting new state-of-the-art benchmarks on the SQuAD dataset by significantly improving accuracy in machine comprehension tasks.

With the emergence of Transformer, the attention mechanism has been more widely used and developed:

【Meshed-Memory Transformer for Image Captioning】addressed the challenge of enhancing image captioning by improving the integration of visual and textual data, as existing models were insufficient in capturing complex inter-modal relationships. They proposed the Meshed-Memory Transformer (M2 Transformer), a novel architecture that utilizes a memory-augmented encoder and a meshed decoder to exploit both low- and high-level visual features while incorporating learned a priori knowledge. The M2 Transformer achieved state-of-the-art performance on the COCO dataset, demonstrating its effectiveness in handling novel objects and setting new benchmarks for image captioning tasks.

With the more efficiently encoded image and text inputs provided by the pre-trained model, the ability of cross-modal retrieval can be further improved:

【Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning】address the limitations of relying solely on pre-trained object detectors for image captioning, which often miss crucial contextual relationships and scene details. They propose an enhanced approach utilizing the CLIP model for cross-modal retrieval of textual descriptions that supplement the detected visual content, and an image conditioning module to dynamically adjust feature representations. Their methodology significantly improves the descriptive accuracy, achieving a notable increase of 7.5% in CIDEr and 1.3% in BLEU-4 scores over state-of-the-art models, demonstrating its effectiveness in producing more contextually rich and grounded captions.

In addition, regarding the attention mechanism, in addition to Transformer, there are other ideas for capturing high-order attention:

【X-Linear Attention Networks for Image Captioning】aim to address how to effectively integrate visual and textual information in image caption generation, specifically how to enhance model performance through high-order feature interactions. A new attention module, the X-Linear Attention Block, is proposed. This module fully utilizes bilinear pooling to capture second-order interactions between either single-modal or multimodal features, and by stacking multiple attention blocks, it simulates higher-order and even infinite-order feature interactions. On the COCO dataset, the newly proposed X-Linear Attention Network (X-LAN) achieved unprecedented performance, particularly on the CIDEr evaluation metric, demonstrating its effectiveness in handling complex multimodal reasoning tasks in image caption generation.

And Mamba, a new sequence modeling architecture based on Selective State Space Models (SSM), aims to solve the computational efficiency problem of Transformer in long sequence processing:

【Mamba: Linear-Time Sequence Modeling with Selective State Spaces】aim to address the computational inefficiency of Transformer-based architectures in handling long sequences, particularly their quadratic scaling with respect to sequence length and their inability to model content-based reasoning effectively. The authors introduce a new class of models called Selective State Space Models (SSMs), incorporated into a simplified architecture named Mamba. This model selectively propagates or forgets information based on the current input, allowing for linear-time sequence modeling and improved content-based reasoning, without relying on attention mechanisms. The Mamba model demonstrates significant improvements over traditional Transformers, achieving state-of-the-art performance across various modalities such as language, audio, and genomics. It outperforms Transformers of equivalent size and matches the performance of much larger models, particularly in handling long sequences and real data with lengths up to a million tokens.

The idea of this study is to further specialize the application of attention mechanism in VQA problems, that is, attention for vision and question respectively: spatial perception and question perception

【SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers】aimed to address the limitations of both CNNs and Transformers in capturing diverse spatial features efficiently, specifically targeting the inefficiency of large dense kernels and the challenges in capturing local features with large receptive fields. They introduced the Spatial Cross-scale Convolution Module (SCSC), which utilizes a wide range of kernel sizes and an efficient spatial embedding module to capture both microscopic and macroscopic feature representations while maintaining computational efficiency. The SCSC module demonstrated superior performance across various tasks, improving accuracy while significantly reducing computational costs and model parameters, making traditional CNNs competitive with advanced Transformers.

【Question Aware Vision Transformer for Multimodal Reasoning】target the limitation in existing Vision-Language (VL) models, where the vision encoder processes images independently of the associated textual query, leading to suboptimal multimodal reasoning performance. The authors propose Question Aware Vision Transformer (QA-ViT), a model-agnostic approach that embeds question awareness directly into the vision encoder, allowing it to generate visual features that are dynamically aligned with the specific query. QA-ViT consistently improves performance across various VL models and benchmarks, enhancing both visual question answering and image captioning tasks, and outperforms the baseline models, particularly in complex multimodal reasoning scenarios.

The spatial cross-scale convolution module (SCSC) can enhance the performance of convolutional neural networks and visual transformers. By combining multiple receptive fields and deep convolutions, it can improve the representation ability of the model and reduce computational costs. QA-ViT, a visual transformer architecture for multimodal reasoning, integrates the semantic embedding of the problem directly into the visual encoder, enabling it to generate dynamic visual features related to specific problems, thereby enhancing the reasoning ability of the model.
Inspired by these two papers, in the process of visual and text embedding, we plan to use cross-scale spatial convolution modules and question-aware text semantic embedding to simultaneously obtain image space perception and text question perception, hoping to achieve better performance in the modality fusion stage.